
import os
import sys
import json
import streamlit as st
import pandas as pd

# Ensure project root on sys.path so `core/*` is importable when running `app/main.py`
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))



# --- Core imports (these should exist in your repo) ---
from core.file_handler import save_uploaded_file, read_uploaded_file  # type: ignore

# GPT explainer (non-RAG). If missing, we handle gracefully below.
try:
    from core.model_explainer import explain_model  # type: ignore
except Exception:  # noqa: BLE001
    explain_model = None  # fallback handled later

# RAG pipeline (Chroma + OpenAI embeddings)
try:
    from core.rag_chain import run_rag_pipeline  # type: ignore
except Exception:  # noqa: BLE001
    run_rag_pipeline = None

# Tool-calling executor & registry
from core.llm_executor import make_client, chat_with_tools  # type: ignore
from core.tool_registry import register_tool  # type: ignore

# Import tools so decorators run and register (e.g., my_model, run_oga_hdic)
# - core.llm_tools should define @register_tool functions
try:
    import core.llm_tools  # noqa: F401
except Exception:
    pass

# ---- Page config ----
st.set_page_config(page_title="Stat Explainer", layout="wide")
st.title("ğŸ“Š stat-explainer â€” LLM å¹³å° (RAG + Tools)")

# ---- Helper: obtain API key (Streamlit Cloud secrets first, else env) ----
def get_api_key() -> str | None:
    return st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))

API_KEY = get_api_key()
if not API_KEY:
    st.warning(
        "æ‰¾ä¸åˆ° OPENAI_API_KEYã€‚è«‹åœ¨ Streamlit Secrets æˆ–ç’°å¢ƒè®Šæ•¸ä¸­è¨­å®šï¼Œä»¥å•Ÿç”¨ GPT / RAG / å·¥å…·å‘¼å«ã€‚"
    )

# =============================================================
# Section 1 â€” Upload & Preview
# =============================================================
st.header("1) ä¸Šå‚³èˆ‡é è¦½ Upload & Preview")

uploaded_file = st.file_uploader("è«‹ä¸Šå‚³è³‡æ–™/æ¨¡å‹è¼¸å‡ºæª”ï¼ˆcsv / json / pklï¼‰", type=["csv", "json", "pkl"], key="data")
preview: pd.DataFrame | None = None
file_path: str | None = None

if uploaded_file:
    file_path = save_uploaded_file(uploaded_file)
    st.success(f"âœ… å·²å„²å­˜è‡³: {file_path}")
    preview = read_uploaded_file(file_path)
    st.subheader("ğŸ“‹ æª”æ¡ˆé è¦½")
    if isinstance(preview, pd.DataFrame):
        st.dataframe(preview, use_container_width=True)
        st.caption(f"Rows: {preview.shape[0]} | Cols: {preview.shape[1]}")
    else:
        st.write(preview)

# =============================================================
# Section 2 â€” GPT Explanation (non-RAG)
# =============================================================
st.header("2) GPT è§£é‡‹ï¼ˆä¸å« RAGï¼‰")
model_choice = st.selectbox("é¸æ“‡æ¨¡å‹ï¼ˆåƒ…ç”¨æ–¼å±•ç¤ºï¼›explain_model å…§éƒ¨å¯å¿½ç•¥ï¼‰", ["gpt-4o", "gpt-4o-mini"], index=0)
if st.button("ğŸ“– ç”¢ç”Ÿè§£é‡‹", disabled=preview is None):
    if preview is None:
        st.info("è«‹å…ˆä¸Šå‚³æª”æ¡ˆã€‚")
    elif not API_KEY:
        st.error("ç¼ºå°‘ OPENAI_API_KEYï¼Œç„¡æ³•å‘¼å« GPTã€‚")
    else:
        try:
            if explain_model is None:
                # Lightweight fallback: summarize first rows as text
                text_block = preview.head(50).to_csv(index=False) if isinstance(preview, pd.DataFrame) else str(preview)
                st.warning("æœªç™¼ç¾ core.model_explainer.explain_modelï¼Œä½¿ç”¨ç°¡æ˜“é¡¯ç¤ºç‚ºå¾Œå‚™æ–¹æ¡ˆã€‚")
                st.text_area("æš«æ™‚è¼¸å‡º", value=text_block, height=300)
            else:
                # explain_model expects textual content
                csv_text = preview.to_csv(index=False) if isinstance(preview, pd.DataFrame) else str(preview)
                out = explain_model(csv_text, file_type="csv")  # type: ignore[arg-type]
                st.text_area("ğŸ” GPT è§£é‡‹çµæœ", value=out, height=320)
        except Exception as e:  # noqa: BLE001
            st.error(f"è§£é‡‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

# =============================================================
# Section 3 â€” RAG: upload .txt background + augment
# =============================================================
st.header("3) RAG å¢å¼·ï¼ˆä¸Šå‚³ .txt èƒŒæ™¯èªªæ˜ â†’ æª¢ç´¢ + è§£é‡‹)")
rag_file = st.file_uploader("ä¸Šå‚³èƒŒæ™¯èªªæ˜æª”ï¼ˆç´”æ–‡å­— .txtï¼‰", type=["txt"], key="rag")
rag_text: str | None = None
if rag_file is not None:
    try:
        rag_text = rag_file.read().decode("utf-8", errors="ignore")
    except Exception:  # noqa: BLE001
        rag_text = None

if rag_text:
    st.text_area("ğŸ“ èƒŒæ™¯å…§å®¹é è¦½", rag_text[:5000], height=180)

col_rag1, col_rag2 = st.columns([1, 2])
with col_rag1:
    rag_kick = st.button("ğŸ“– ä½¿ç”¨ RAG è§£é‡‹æ¨¡å‹", disabled=(rag_text is None or preview is None))
with col_rag2:
    st.caption("èªªæ˜ï¼šæœƒå»ºç«‹/è¼‰å…¥å‘é‡åº«ï¼ˆChromaï¼‰ï¼Œç”¨æª¢ç´¢ç‰‡æ®µè¼”åŠ© GPT ç”Ÿæˆè§£é‡‹ã€‚")

if rag_kick:
    if not API_KEY:
        st.error("ç¼ºå°‘ OPENAI_API_KEYï¼Œç„¡æ³•åŸ·è¡Œ RAGã€‚")
    elif preview is None:
        st.info("è«‹å…ˆä¸Šå‚³è³‡æ–™æª”æ¡ˆã€‚")
    elif run_rag_pipeline is None:
        st.error("æœªç™¼ç¾ core.rag_chain.run_rag_pipelineï¼Œè«‹ç¢ºèªæ¨¡çµ„å·²å»ºç«‹ã€‚")
    else:
        try:
            # Basic prompt to couple with RAG context
            question = "è«‹æ ¹æ“šèƒŒæ™¯èªªæ˜èˆ‡ä¸Šå‚³è³‡æ–™ï¼Œè§£é‡‹é€™ä»½æ¨¡å‹/è³‡æ–™çš„é‡é»èˆ‡çµè«–ã€‚"
            response = run_rag_pipeline(question=question, raw_text=rag_text)  # type: ignore[arg-type]
            st.text_area("ğŸ” GPTï¼ˆRAGï¼‰å›æ‡‰", value=response, height=320)
        except Exception as e:  # noqa: BLE001
            st.error(f"RAG åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")

# =============================================================
# Section 4 â€” LLM Tool Calling (demo)
# =============================================================
st.header("4) LLM å·¥å…·å‘¼å«ï¼ˆFunction Callingï¼‰æ¸¬è©¦")

# Simple demo tool is defined in core.llm_tools as `my_model`.
# We'll let LLM decide to call it.
with st.expander("é–‹å•Ÿ / æ”¶åˆï¼šæœ€å°å·¥å…·å‘¼å«æ¸¬è©¦"):
    x_val = st.number_input("x", value=3.0, step=1.0)
    y_val = st.number_input("y", value=5.0, step=1.0)
    question = st.text_input(
        "ä½ çš„å•é¡Œ",
        "è«‹ç”¨å¯ç”¨çš„å·¥å…·è¨ˆç®—ç•¶ x=3, y=5 æ™‚çš„æ¨¡å‹è¼¸å‡ºï¼Œä¸¦ç°¡è¦èªªæ˜ã€‚",
    )
    model_name = st.selectbox("é¸æ“‡æ¨¡å‹", ["gpt-4o-mini", "gpt-4o"], index=0, key="tool_model")

    if st.button("ğŸš€ åŸ·è¡Œå·¥å…·æ¸¬è©¦"):
        if not API_KEY:
            st.error("ç¼ºå°‘ OPENAI_API_KEYã€‚")
        else:
            try:
                client = make_client(API_KEY)
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯æ“…é•·çµ±è¨ˆå»ºæ¨¡èˆ‡æ•¸å€¼é‹ç®—çš„åŠ©ç†ã€‚"},
                    {"role": "user", "content": question},
                    {"role": "user", "content": f"çµ¦å®š x={x_val}, y={y_val}"},
                    {"role": "user", "content": "è‹¥éœ€è¦è¨ˆç®—ï¼Œè«‹å‘¼å« my_model ä¸¦å‚³å…¥åƒæ•¸ã€‚"},
                ]
                out = chat_with_tools(client, messages, model=model_name, temperature=0.2)
                st.success("LLM æœ€çµ‚å›è¦†ï¼š")
                st.write(out["content"])
                with st.expander("ğŸ”§ å·¥å…·å‘¼å«æ˜ç´°ï¼ˆdebugï¼‰"):
                    st.json(out["tool_results"])    
            except Exception as e:  # noqa: BLE001
                st.error(f"åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")

# =============================================================
# Section 5 â€” OGA-HDiC (custom function) via tool calling
# =============================================================
st.header("5) è‡ªè¨‚å‡½å¼ï¼šOGA-HDiCï¼ˆé¸æ“‡ y æ¬„ + åƒæ•¸ï¼‰")

if preview is None or not isinstance(preview, pd.DataFrame):
    st.info("è«‹å…ˆæ–¼ä¸Šæ–¹ä¸Šå‚³è³‡æ–™æª”æ¡ˆï¼ˆcsv/json/pklï¼‰ã€‚")
else:
    df = preview
    cols = list(df.columns)
    if not cols:
        st.warning("è³‡æ–™æ²’æœ‰æ¬„ä½å¯ä¾›é¸æ“‡ã€‚")
    else:
        y_col = st.selectbox("é¸æ“‡ç›®æ¨™è®Šæ•¸ y æ¬„ä½", options=cols, index=0)
        default_x = [c for c in cols if c != y_col]
        x_cols = st.multiselect(
            "é¸æ“‡ç‰¹å¾µæ¬„ä½ Xï¼ˆé è¨­ç‚º y ä»¥å¤–å…¨éƒ¨ï¼‰",
            options=cols,
            default=default_x,
        )
        with st.expander("åƒæ•¸è¨­å®š", expanded=False):
            Kn = st.number_input("Knï¼ˆ0=ä¸æŒ‡å®šï¼‰", value=0, min_value=0, step=1)
            c1 = st.number_input("c1", value=5.0, step=0.5)
            HDIC_Type = st.selectbox("HDIC_Type", options=["HDBIC", "HDHQ", "HDAIC"], index=0)
            c2 = st.number_input("c2", value=2.0, step=0.5)
            c3 = st.number_input("c3", value=2.01, step=0.01)
            intercept = st.checkbox("intercept", value=True)
        max_rows = st.number_input("é€å¾€ LLM çš„è³‡æ–™ç­†æ•¸ä¸Šé™", value=1000, min_value=50, step=50)
        data_records = df.iloc[: int(max_rows)].to_dict(orient="records")
        model_name2 = st.selectbox("é¸æ“‡æ¨¡å‹ (tool calling)", ["gpt-4o-mini", "gpt-4o"], index=0, key="oga_model")

        if st.button("ğŸš€ è®“ LLM åŸ·è¡Œ OGA-HDiC"):
            if not API_KEY:
                st.error("ç¼ºå°‘ OPENAI_API_KEYã€‚")
            else:
                try:
                    client = make_client(API_KEY)
                    Kn_arg = None if Kn == 0 else int(Kn)

                    user_goal = (
                        "è«‹åœ¨æä¾›çš„è³‡æ–™ä¸ŠåŸ·è¡Œ OGA-HDiC æ¨¡å‹é¸æ“‡ï¼Œä¸¦èªªæ˜é¸åˆ°çš„è®Šæ•¸èˆ‡çµæœé‡é»ã€‚"
                        "è‹¥éœ€è¦è¨ˆç®—ï¼Œè«‹å‘¼å«å·¥å…· run_oga_hdicï¼Œä¸¦å¡«å…¥ data/y_col/x_cols/Kn/c1/HDIC_Type/c2/c3/interceptã€‚"
                    )

                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ç†Ÿæ‚‰é«˜ç¶­æ¨¡å‹é¸æ“‡çš„çµ±è¨ˆåŠ©ç†ã€‚"},
                        {"role": "user", "content": user_goal},
                        {"role": "user", "content": (
                            f"y_col={y_col}ï¼›x_cols={x_cols}ï¼›Kn={Kn_arg}ï¼›c1={c1}ï¼›"
                            f"HDIC_Type={HDIC_Type}ï¼›c2={c2}ï¼›c3={c3}ï¼›intercept={intercept}ã€‚"
                        )},
                        {"role": "user", "content": f"è³‡æ–™å…±æœ‰ {len(df)} åˆ—ï¼Œé€™è£¡æä¾›å‰ {len(data_records)} åˆ—ç”¨æ–¼è¨ˆç®—ã€‚"},
                        {"role": "user", "content": json.dumps({"data": data_records}, ensure_ascii=False)},
                    ]

                    out = chat_with_tools(client, messages, model=model_name2, temperature=0.1)
                    st.success("LLMï¼ˆå«å·¥å…·åŸ·è¡Œï¼‰å›è¦†ï¼š")
                    st.write(out["content"])
                    with st.expander("ğŸ”§ å·¥å…·å‘¼å«æ˜ç´°ï¼ˆçµæœåŸå§‹ JSONï¼‰"):
                        st.json(out["tool_results"])
                except Exception as e:  # noqa: BLE001
                    st.error(f"OGA-HDiC åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")

# -------------------------------------------------------------
# End of file
# -------------------------------------------------------------
